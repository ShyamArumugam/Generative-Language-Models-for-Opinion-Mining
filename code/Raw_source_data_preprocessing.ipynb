{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  12\n",
      "Names of files read:  ['cafemom.json', 'chicagotribune.json', 'disqus.json', 'huffingtonpost.json', 'latimes.json', 'nypost.json', 'nytimes.json', 'quora.json', 'reddit.json', 'usatoday.json', 'usmessageboard.json', 'washingtonpost.json']\n",
      "1  files read , count:  (2206, 11)\n",
      "2  files read , count:  (2487, 11)\n",
      "3  files read , count:  (10680, 11)\n",
      "4  files read , count:  (10680, 11)\n",
      "5  files read , count:  (11054, 11)\n",
      "6  files read , count:  (11054, 11)\n",
      "7  files read , count:  (27182, 11)\n",
      "8  files read , count:  (36788, 11)\n",
      "9  files read , count:  (46079, 11)\n",
      "10  files read , count:  (46338, 11)\n",
      "11  files read , count:  (124641, 11)\n",
      "12  files read , count:  (209310, 11)\n",
      "csv export completed\n",
      "Final count:  (209310, 11)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> english --> unbiased (forum + newssites) data\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/english/unbiased'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title']          \n",
    "            article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "                \n",
    "                \n",
    "                # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "# Loading Dataframe from pkl\n",
    "# jsons_data = pd.read_pickle(final_df)        \n",
    "        \n",
    "output_filename = path_to_json + \"/output/final_df.csv\"            \n",
    "jsons_data.to_csv(output_filename, index = None, header=True, encoding='utf-8-sig')        \n",
    "\n",
    "# print(jsons_data stats)\n",
    "print('csv export completed')\n",
    "print('Final count: ', jsons_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  5\n",
      "Names of files read:  ['cafemom.json', 'disqus.json', 'quora.json', 'reddit.json', 'usmessageboard.json']\n",
      "1   cafemom.json  files read , count:  (2206, 12)\n",
      "2   disqus.json  files read , count:  (10399, 12)\n",
      "3   quora.json  files read , count:  (20005, 12)\n",
      "4   reddit.json  files read , count:  (29296, 12)\n",
      "5   usmessageboard.json  files read , count:  (107599, 12)\n",
      "df pickling completed\n",
      "Final count:  (107599, 12)\n",
      "Final only comments count:  (106893, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (26065, 2)\n",
      "Final relevant conv comments count:  (16305, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> english --> unbiased --> Forum data\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/english/unbiased/Forum'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title'] \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"            \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "                \n",
    "                \n",
    "                # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_eng_unbiased_forum_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final count:  (107599, 12)\n",
      "Final only comments count:  (106893, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (26049, 2)\n",
      "Final relevant conv comments count:  (16296, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> english --> unbiased --> Forum data\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/english/unbiased/Forum'\n",
    "\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "\n",
    "# Loading Dataframe from pkl\n",
    "jsons_data = pd.read_pickle(final_df)        \n",
    "\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. '}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('  ', ' ', re.sub(r'[\\p{Devanagari}\\p{P}\\|]+', ' ', x).strip()) if bool(re.search(r'\\p{Devanagari}+',x)) else re.sub(r'\\!+', \"!\", re.sub(r'\\.+', \".\", x))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_eng_unbiased_forum_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  7\n",
      "Names of files read:  ['chicagotribune.json', 'huffingtonpost.json', 'latimes.json', 'nypost.json', 'nytimes.json', 'usatoday.json', 'washingtonpost.json']\n",
      "1   chicagotribune.json  files read , count:  (281, 12)\n",
      "2   huffingtonpost.json  files read , count:  (281, 12)\n",
      "3   latimes.json  files read , count:  (655, 12)\n",
      "4   nypost.json  files read , count:  (655, 12)\n",
      "5   nytimes.json  files read , count:  (16783, 12)\n",
      "6   usatoday.json  files read , count:  (17042, 12)\n",
      "7   washingtonpost.json  files read , count:  (101711, 12)\n",
      "df pickling completed\n",
      "Final count:  (101711, 12)\n",
      "Final only comments count:  (101711, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (26491, 2)\n",
      "Final relevant conv comments count:  (12301, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> english --> unbiased --> newssites data\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/english/unbiased/newssites'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title']   \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"     \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "                \n",
    "                \n",
    "                # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "# print(jsons_data stats)\n",
    "# print('txt export completed')\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_eng_unbiased_news_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  11\n",
      "Names of files read:  ['bfriendsBrigitte.json', 'chefkoch.json', 'forum_ernaehrung.json', 'glamour.json', 'gutefrage.json', 'kleiderkreisel.json', 'paradisi.json', 'reddit_de.json', 'schule-und-familie.json', 'webkoch.json', 'werweisswas.json']\n",
      "1   bfriendsBrigitte.json  files read , count:  (2898, 12)\n",
      "2   chefkoch.json  files read , count:  (12702, 12)\n",
      "3   forum_ernaehrung.json  files read , count:  (12702, 12)\n",
      "4   glamour.json  files read , count:  (12989, 12)\n",
      "5   gutefrage.json  files read , count:  (18994, 12)\n",
      "6   kleiderkreisel.json  files read , count:  (23825, 12)\n",
      "7   paradisi.json  files read , count:  (23888, 12)\n",
      "8   reddit_de.json  files read , count:  (25553, 12)\n",
      "9   schule-und-familie.json  files read , count:  (25581, 12)\n",
      "10   webkoch.json  files read , count:  (25615, 12)\n",
      "11   werweisswas.json  files read , count:  (25856, 12)\n",
      "df pickling completed\n",
      "Final count:  (25856, 12)\n",
      "Final only comments count:  (25836, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (12654, 2)\n",
      "Final relevant conv comments count:  (2840, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> german --> unbiased --> forum data\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/german/unbiased/forums'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title'] \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"     \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "                \n",
    "                # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_ger_unbiased_forum_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  29\n",
      "Names of files read:  ['aargauer.json', 'br.json', 'derstandard.json', 'diepresse.json', 'focus.json', 'freitag.json', 'handelsblatt.json', 'heise.json', 'huffingtonpost_de.json', 'krone.json', 'kurier.json', 'luzernzeitung.json', 'merkur.json', 'nachrichtenat.json', 'ndr.json', 'nzz.json', 'rp.json', 'salzburgcom.json', 'spiegel.json', 'srf.json', 'swr.json', 'tagesanzeiger.json', 'tagesschau.json', 'tagesspiegel.json', 'taz.json', 'waz.json', 'wdr.json', 'welt.json', 'zeit.json']\n",
      "1   aargauer.json  files read , count:  (397, 12)\n",
      "2   br.json  files read , count:  (783, 12)\n",
      "3   derstandard.json  files read , count:  (81498, 12)\n",
      "4   diepresse.json  files read , count:  (84513, 12)\n",
      "5   focus.json  files read , count:  (90319, 12)\n",
      "6   freitag.json  files read , count:  (90448, 12)\n",
      "7   handelsblatt.json  files read , count:  (91371, 12)\n",
      "8   heise.json  files read , count:  (95007, 12)\n",
      "9   huffingtonpost_de.json  files read , count:  (95007, 12)\n",
      "10   krone.json  files read , count:  (95007, 12)\n",
      "11   kurier.json  files read , count:  (95877, 12)\n",
      "12   luzernzeitung.json  files read , count:  (95877, 12)\n",
      "13   merkur.json  files read , count:  (96576, 12)\n",
      "14   nachrichtenat.json  files read , count:  (98568, 12)\n",
      "15   ndr.json  files read , count:  (98568, 12)\n",
      "16   nzz.json  files read , count:  (99190, 12)\n",
      "17   rp.json  files read , count:  (100998, 12)\n",
      "18   salzburgcom.json  files read , count:  (100998, 12)\n",
      "19   spiegel.json  files read , count:  (163858, 12)\n",
      "20   srf.json  files read , count:  (165335, 12)\n",
      "21   swr.json  files read , count:  (165335, 12)\n",
      "22   tagesanzeiger.json  files read , count:  (170207, 12)\n",
      "23   tagesschau.json  files read , count:  (174584, 12)\n",
      "24   tagesspiegel.json  files read , count:  (178119, 12)\n",
      "25   taz.json  files read , count:  (183656, 12)\n",
      "26   waz.json  files read , count:  (185483, 12)\n",
      "27   wdr.json  files read , count:  (185483, 12)\n",
      "28   welt.json  files read , count:  (186933, 12)\n",
      "29   zeit.json  files read , count:  (195429, 12)\n",
      "df pickling completed\n",
      "Final count:  (195429, 12)\n",
      "Final only comments count:  (191745, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (89186, 2)\n",
      "Final relevant conv comments count:  (59665, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> german --> unbiased --> newssites data\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/german/unbiased/newssites'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title']  \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"     \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "              # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_ger_unbiased_news_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  5\n",
      "Names of files read:  ['fb.json', 'foodbabe.json', 'foodrevolution.json', 'organicauthority.json', 'organicconsumers.json']\n",
      "1   fb.json  files read , count:  (299126, 12)\n",
      "2   foodbabe.json  files read , count:  (303070, 12)\n",
      "3   foodrevolution.json  files read , count:  (306037, 12)\n",
      "4   organicauthority.json  files read , count:  (306037, 12)\n",
      "5   organicconsumers.json  files read , count:  (306037, 12)\n",
      "df pickling completed\n",
      "Final count:  (306037, 12)\n",
      "Final only comments count:  (306036, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (75866, 2)\n",
      "Final relevant conv comments count:  (18475, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> english --> biased \n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/english/biased'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title'] \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"     \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "            \n",
    "                # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_eng_biased_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  11\n",
      "Names of files read:  ['berlinbio.json', 'biologisch-lecker.json', 'campact.json', 'drfeil.json', 'eatsmarter.json', 'greenpeace.json', 'individualisten.json', 'karmakonsum.json', 'lebeheute.json', 'nachhaltigleben.json', 'scilogs.json']\n",
      "1   berlinbio.json  files read , count:  (6, 12)\n",
      "2   biologisch-lecker.json  files read , count:  (16, 12)\n",
      "3   campact.json  files read , count:  (2593, 12)\n",
      "4   drfeil.json  files read , count:  (2611, 12)\n",
      "5   eatsmarter.json  files read , count:  (3790, 12)\n",
      "6   greenpeace.json  files read , count:  (3792, 12)\n",
      "7   individualisten.json  files read , count:  (3817, 12)\n",
      "8   karmakonsum.json  files read , count:  (3827, 12)\n",
      "9   lebeheute.json  files read , count:  (3833, 12)\n",
      "10   nachhaltigleben.json  files read , count:  (3916, 12)\n",
      "11   scilogs.json  files read , count:  (5791, 12)\n",
      "df pickling completed\n",
      "Final count:  (5791, 12)\n",
      "Final only comments count:  (5791, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (1628, 2)\n",
      "Final relevant conv comments count:  (315, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> german --> biased --> blogs\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/german/biased/blogs'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title'] \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"     \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "\n",
    "                # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "# print(jsons_data stats)\n",
    "# print('txt export completed')\n",
    "print('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_ger_biased_blog_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files read:  2\n",
      "Names of files read:  ['biooekoforum.json', 'utopia.json']\n",
      "1   biooekoforum.json  files read , count:  (15, 12)\n",
      "2   utopia.json  files read , count:  (374, 12)\n",
      "df pickling completed\n",
      "Final count:  (374, 12)\n",
      "Final only comments count:  (362, 12)\n",
      "relevant comments export completed\n",
      "Final relevant comments count:  (306, 2)\n",
      "Final relevant conv comments count:  (126, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading organic_dataset --> raw_source_data --> german --> biased -->forums\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'C:/Users/a_shy/Desktop/TUM/Sem 4/Application Project/data/organic_dataset/raw_source_data/german/biased/forums'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "print('No. of files read: ', len(json_files))\n",
    "print('Names of files read: ', json_files)\n",
    "\n",
    "# here I define my pandas Dataframe with the columns I want to get from the json\n",
    "jsons_data = pd.DataFrame(columns=['file_name', 'article_author_id', 'article_title', 'comment_id', 'comment_author_id', 'comment_author_name', 'comment_text', 'comment_rating', 'comment_replyTo', 'resource_type', 'relevant', 'is_conversation'])\n",
    "\n",
    "i = 0     # for Dataframe row\n",
    "j = 1     # for len(json_files)\n",
    "\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        for data in json_text:\n",
    "            article_title = data['article_title']  \n",
    "            article_author_id = \" \"\n",
    "            for authordata in data['article_author']:\n",
    "                article_author_id = authordata['article_author_id'] if 'article_author_id' in authordata else \" \"     \n",
    "#             article_author_id = data['article_author'][0]['article_author_id'] if 'article_author_id' in data['article_author'] else \" \"            \n",
    "            resource_type =  data['resource_type']\n",
    "            relevant = data['relevant']\n",
    "            \n",
    "            for commentdata in data['comments']:\n",
    "                \n",
    "                comment_id = commentdata['comment_id']\n",
    "                comment_author_id = commentdata['comment_author']['comment_author_id'] if 'comment_author_id' in commentdata['comment_author'] else \" \"\n",
    "                comment_author_name = commentdata['comment_author']['comment_author_name'] if 'comment_author_name' in commentdata['comment_author'] else \" \"\n",
    "                comment_text = commentdata['comment_text']\n",
    "                comment_rating = commentdata['comment_rating'] if 'comment_rating' in commentdata else \" \"\n",
    "                comment_replyTo = commentdata['comment_replyTo'] if 'comment_replyTo' in commentdata else \" \"\n",
    "                is_conversation = 1 if 'comment_replyTo' in commentdata else 0\n",
    "                \n",
    "               # here I push a list of data into a pandas DataFrame at row given by 'i'\n",
    "                jsons_data.loc[i] = [js, article_author_id, article_title, comment_id, comment_author_id, comment_author_name, comment_text, comment_rating, comment_replyTo, resource_type, relevant, is_conversation]\n",
    "                i = i + 1\n",
    "            \n",
    "        print(j, ' ', js, ' files read , count: ', jsons_data.shape)\n",
    "        j = j + 1\n",
    "\n",
    "        \n",
    "# Saving Dataframe to pkl\n",
    "\n",
    "final_df = path_to_json + \"/output/final_df.pkl\" \n",
    "jsons_data.to_pickle(final_df)\n",
    "print('df pickling completed')\n",
    "\n",
    "rint('Final count: ', jsons_data.shape)\n",
    "print('Final only comments count: ', jsons_data[jsons_data.comment_text!=''].shape)\n",
    "\n",
    "#Filtering relevant comments\n",
    "json_filtered = pd.DataFrame(jsons_data[(jsons_data.relevant==1) & (jsons_data.comment_text!='') & (jsons_data.comment_text.apply(len) > 15)][['comment_text','is_conversation']])\n",
    "\n",
    "\n",
    "json_filtered.replace({ r'\\A\\s+|\\s+\\Z': '', '\\n' : '. ', r'\\.+': '.', r'\\,+': ',', r'\\-+': '-', r\"\\'+\": \"'\", r'\\!+': '!', r'\\?+': '?'}, regex=True, inplace=True)\n",
    "\n",
    "json_filtered['comment_text'] = [(re.sub('\\  +', ' ', (' '.join([(' ' if bool(re.search(r'\\p{Devanagari}',y)) else y.strip()) for y in x.split()]).strip() if bool(re.search(r'\\p{Devanagari}+',x)) else x.strip()))) for x in json_filtered['comment_text']]\n",
    "\n",
    "fil_output_txt_filename = path_to_json + \"/output/train_ger_biased_forum_rel.txt\" \n",
    "json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)]['comment_text'].to_csv(fil_output_txt_filename, header=False, index=False, sep='\\t', mode='w+')\n",
    "\n",
    "# print(relevant comments stats)\n",
    "print('relevant comments export completed')\n",
    "print('Final relevant comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15)].shape)\n",
    "\n",
    "\n",
    "# json_filtered_conv_rel = pd.DataFrame(json_filtered[(jsons_data.is_conversation==1)])\n",
    "print('Final relevant conv comments count: ', json_filtered[(json_filtered.comment_text!='') & (json_filtered.comment_text.apply(len) > 15) & (json_filtered.is_conversation==1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
